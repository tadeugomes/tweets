count(word, sort = TRUE) %>%
left_join(base_limpa %>%
group_by(empresa) %>%
summarise(total = n())) %>%
mutate(freq = n/total)
head(frequencia,10)
frequencia_2 <- frequencia %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
frequencia
frequencia %>%
count(word, sort=TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
theme_bw()
frequencia %>%
count(word, sort=TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
theme_bw()
head(frequencia,10)
frequencia %>%
count(word, sort=TRUE) %>%
top_n(15) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
coord_flip() +
theme_bw()
frequencia %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
coord_flip() +
theme_bw()
base_unica %>%
group_by(empresa) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = empresa)) +
geom_col(show.legend = FALSE) +
facet_wrap(~empresa, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
head(frequencia_tidy,5)
base_limpa_2 <- base_limpa %>%
filter(created_at >= as.Date("2022-03-06"),
created_at < as.Date("2022-03-01"))
uso_palavras <- base_limpa %>%
filter(!str_detect(word, "^@")) %>%
count(word, empresa) %>%
group_by(word) %>%
filter(sum(n) >= 10) %>%
ungroup() %>%
spread(empresa, n, fill = 0) %>%
mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%
mutate(logratio = log(visa / master)) %>%
arrange(desc(logratio))
uso_palavras %>%
arrange(abs(logratio))
#Plotar palavras mais usadas por cada grupo
uso_palavras %>%
group_by(logratio < 0) %>%
top_n(15, abs(logratio)) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio, fill = logratio < 0)) +
geom_col(show.legend = TRUE) +
coord_flip() +
ylab("Taxa log odds (visa/master)") +
scale_fill_discrete(name = "", labels = c("visa", "master"))
#Plotar palavras mais usadas
uso_palavras %>%
group_by(logratio < 0) %>%
top_n(15, abs(logratio)) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio, fill = logratio < 0)) +
geom_col(show.legend = TRUE) +
coord_flip() +
ylab("Taxa log odds (visa/master)") +
scale_fill_discrete(name = "", labels = c("visa", "master"))
# remove elementos e limpa
master$stripped_text <- gsub("http.*","",  master$text)
master$stripped_text <- gsub("https.*","", master$stripped_text)
master_clean <- master %>%
dplyr::select(stripped_text) %>%
unnest_tokens(word, stripped_text)
# Limpar stopwords
master_clean <- cacho_clean %>%
anti_join(get_stopwords(language = "en"))
# Limpar stopwords
master_clean <- master_clean %>%
anti_join(get_stopwords(language = "en"))
master_clean %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(y = "Count",
x = "Palavras únicas",
title = "Contagem de Palavras únicas tuitadas sobre #mastercard",
subtitle = "Gráfico com stopwords removidas")
master_clean %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(y = "Frequência",
x = "Palavras únicas",
title = "Contagem de palavras únicas tuitadas sobre #mastercard",
subtitle = "As palavras se relacionam diretamente ao confito bélico")+
theme_bw()
master_pair <- master %>%
dplyr::select(stripped_text) %>%
unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)
master_pair %>%
count(paired_words, sort = TRUE)
master_separar_palavras <- master_pair %>%
separate(paired_words, c("word1", "word2"), sep = " ")
master_filtro <- master_separar_palavras %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# Nova contagem de bigramns
master_freq <- master_filtro %>%
count(word1, word2, sort = TRUE)
head(master_freq)
library(igraph)
library(ggraph)
# (Plotagem de pontes esta quebrada)
master_freq %>%
filter(n >= 3) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n))+
#geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "#todecacho: pares de palavras associadas em Tweets",
subtitle = "Indica a frequência de uso associado das palavras",
x = "", y = "")
# (Plotagem de pontes esta quebrada)
master_freq %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n))+
#geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "#todecacho: pares de palavras associadas em Tweets",
subtitle = "Indica a frequência de uso associado das palavras",
x = "", y = "")
# (Plotagem de pontes esta quebrada)
master_freq %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n))+
#geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Pares de palavras associadas à Mastercard no contexto bélico",
subtitle = "Indica a frequência de uso associado das palavras",
x = "", y = "", colour = "Freq")
master_fav<-master %>%
select(text, favorite_count, retweet_count) %>%
arrange(desc(favorite_count))
View(master_fav)
visa_fav<-visa %>%
select(text, favorite_count, retweet_count) %>%
arrange(desc(favorite_count))
visa_fav %>%
arrange(-retweet_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count)
visa %>%
arrange(-retweet_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count)
visa %>%
arrange(-retweet_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count)
visa %>%
arrange(-retweet_count) %>%
slice(2) %>%
select(created_at, screen_name, text, retweet_count)
visa %>%
arrange(-retweet_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count)
master %>%
arrange(-retweet_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count)
master %>%
arrange(-favorite_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count)
master %>%
arrange(-favorite_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count,favorite_count)
master %>%
arrange(-favorite_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count,favorite_count)
master_fav %>%
count(screen_name, sort = TRUE) %>%
top_n(10) %>%
mutate(screen_name = paste0("@", screen_name))
master_fav<-master %>%
select(created_at, screen_name, text, favorite_count, retweet_count) %>%
arrange(desc(favorite_count))
master_fav %>%
count(screen_name, sort = TRUE) %>%
top_n(10) %>%
mutate(screen_name = paste0("@", screen_name))
master %>%
unnest_tokens(hashtag, text, "tweets", to_lower = FALSE) %>%
filter(str_detect(hashtag, "^#"),
hashtag != "#ClimateEmergency") %>%
count(hashtag, sort = TRUE) %>%
top_n(10)
words <- master %>%
mutate(text = str_remove_all(text, "&amp;|&lt;|&gt;"),
text = str_remove_all(text, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),
text = str_remove_all(text, "[^\x01-\x7F]")) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"),
!str_detect(word, "^#"),
!str_detect(word, "@\\S+")) %>%
count(word, sort = TRUE)
library(wordcloud)
words2 = words[-5,]
words2 %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = "#F29545"))
with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = c("red", "blue"))
words2 %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = c("red", "blue")))
words2 %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = c("red", "blue")))
words2 %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 500, colors = c("red", "blue")))
words2 %>%
with(wordcloud(word, n, random.order = FALSE, colors = c("red", "blue")))
View(words)
words %>%
with(wordcloud(word, n, random.order = FALSE, colors = c("red", "blue")))
library(wordcloud2)
words %>%
with(wordcloud2(word, n, max.words = 100, random.order = FALSE, colors = c("yellow", "blue")))
wordcloud2(words)
wordcloud2(words, backgroundColor = "yeloow")
wordcloud2(words, backgroundColor = "yellow")
letterCloud(words, word = "Ucrania", size = 2)
letterCloud(words, word = "Ucrania", size = 2)
letterCloud(words, word = "UK", size = 1)
wordcloud2(words)
wordcloud2(words, backgroundColor = "blue")
wordcloud2(words, color='random-dark)
### Analise de sentimento
wordcloud2(words, color='random-dark')
wordcloud2(words, color='random-dark')
senti_words <- words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
senti_words %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
View(senti_words)
View(words)
senti_words <- words %>%
inner_join(get_sentiments("bing")) %>%
ungroup()
View(senti_words)
senti_words %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
library(reshape2)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("blue", "yellow"),
max.words = 300)
View(senti_words)
comparison.cloud(senti_words,max.words=200,random.order=FALSE,c(4,0.4), title.size=1.4)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("blue", "yellow"),
random.order=FALSE,c(4,0.4), title.size=1.4)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("blue", "yellow"),
random.order=FALSE,c(4,0.4), title.size=3)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray", "red"),
random.order=FALSE,c(4,0.4), title.size=3)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray", "red"),
random.order=FALSE,c(4,0.4), title.size=5)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray", "red"),
random.order=FALSE,c(4,0.4), title.size=1)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(title.colors=c("dark red","green"),colors = c(" black", "blue"),
max.words = 1500))
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(title.colors=c("dark red","green"),colors = c(" black", "blue"),
max.words = 1500)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(title.colors=c("dark","green"),colors = c(" black", "blue"),
)
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c(" black", "blue"),
max.words = 1500)
?wordcloud
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c(" black", "blue"),
max.words = 1500, scale=c(4,.5))
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c(" black", "blue"),
max.words = 1500, scale=c(4,.5),
title.colors=c("red","green"))
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c(" black", "blue"),
max.words = 1500, scale=c(4,.5),
title.colors=c("red","yellow"))
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray", "blue"),
max.words = 1500, scale=c(8,.3),
title.colors=c("red","dark"))
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray", "blue"),
max.words = 1500, scale=c(8,.3),
title.colors=c("red","#076147"))
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#E8451A", "blue"),
max.words = 1500, scale=c(8,.3),
title.colors=c("red","#076147"))
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#E8451A", "#076147"),
max.words = 1500, scale=c(5,.4),
title.colors=c("red","#076147"))
senti_words %>%
inner_join(get_sentiments("bing")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#E8451A", "#076147"),
max.words = 500, scale=c(5,.4),
title.colors=c("red","#076147"))
write.csv(master, "master")
write.csv(visa, "visa")
# Vamos salvar os dados extraídos do Twitter
write.csv(master, "master.csv")
write.csv(visa, "visa.csv")
View(visa)
write.csv(visa, "visa.csv")
write.csv2(visa, "visa.csv")
View(visa)
save(master, visa, file = "tweets.RData")
save.image(file = "mastercard_files.RData")
ggplot(frequencia_tidy, aes(visa, master)) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
geom_abline(color = "blue")+
theme_bw()+
ggplot2::labs(x = "Visa", y = "Mastercard", title = "Frequência de Tweets associados às operadoras #mastercard e #visa",
subtitle = "Comparação de uso de palavras em cada hashtag",
caption = "\nFonte: Dados coletados pela API do Twitter via rtweet")
master %>%
unnest_tokens(hashtag, text, "tweets", to_lower = FALSE) %>%
filter(str_detect(hashtag, "^#"),
hashtag != "#ClimateEmergency") %>%
count(hashtag, sort = TRUE) %>%
top_n(10)
master_fav %>%
count(screen_name, sort = TRUE) %>%
top_n(10) %>%
mutate(screen_name = paste0("@", screen_name))
visa %>%
arrange(-retweet_count) %>%
slice(1) %>%
select(created_at, screen_name, text, retweet_count)
# (Plotagem de pontes esta quebrada)
master_freq %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n))+
#geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Pares de palavras associadas à Mastercard no contexto bélico",
subtitle = "Indica a frequência de uso associado das palavras",
x = "", y = "", colour = "Freq")
# (Plotagem de pontes esta quebrada)
master_freq %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n))+
geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Pares de palavras associadas à Mastercard no contexto bélico",
subtitle = "Indica a frequência de uso associado das palavras",
x = "", y = "", colour = "Freq")
# (Plotagem de pontes esta quebrada)
master_freq %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
#geom_edge_link(aes(edge_alpha = n, edge_width = n))+
geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
theme_bw()+
labs(title = "Pares de palavras associadas à Mastercard no contexto bélico",
subtitle = "Indica a frequência de uso associado das palavras",
x = "", y = "", colour = "Freq")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
# Pacotes
# Carregar pacote rtweet para acesso aos dados no Twitter
library(rtweet)
# Bibliotecas para manipular dados e criar gráficos
library(ggplot2)
library(dplyr)
# Biblioteca para textmine
library(tidytext)
# Pacote para criar grafos
library(igraph)
library(ggraph)
# Pacote para manipular datas
library(lubridate)
load("mastercard_files.RData")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, eval = TRUE, message = FALSE)
## Vamos criar uma nuvem de palavras
library(knitr)
kable(master %>%
unnest_tokens(hashtag, text, "tweets", to_lower = FALSE) %>%
filter(str_detect(hashtag, "^#"),
hashtag != "#ClimateEmergency") %>%
count(hashtag, sort = TRUE) %>%
top_n(10), caption = "10 hashtags mais frequentes")+
kable_classic(full_width = F, html_font = "Cambria")
View(master_fav)
